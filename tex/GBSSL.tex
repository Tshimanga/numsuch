\documentclass{article}
\usepackage{hyperref}
\begin{document}
\title{Algorithms for Graph-Based Supervised Learning}
\author{Brian Dolan}
\maketitle
Hello

\section{Modified Adsorption}

This routine is described in Talukdar below.

I'm looking for a good parallelization strategy for this function.  I'm going to expand it as far as I can to see if something presents itself.  We are aiming to find $c_v$ for each vertex in the graph.

\begin{eqnarray}
p(a|b) &=& \frac{W_{a,b}}{\sum_u W_{u,b}} \\
H(x) &=& -\sum_y p(y|x) \log p(y|x) \\
&=& -\sum _y \left( \frac{W_{y,x}} {\sum_u W_{u,x} } \right) \log \left(  \frac{W_{y,x}}{\sum_u W_{u,x} } \right) \\
&=& \frac{-1}{\sum_u W_{u,x} } \sum_y W_{y,x} \left[ \log W_{y,x} - \log \sum_u W_{u,x} \right]
\end{eqnarray}

So we can define $$m(x) = \sum_u W_{u,x}$$ and reduce the above to 

\begin{eqnarray}
H(x)  &=& -m(x)^{-1} \sum_y W_{y,x} \left[ \log W_{y,x} - \log m(x) \right] \\
  &=& -m(x)^{-1}  \left[  \sum_y W_{y,x} \log W_{y,x} -  \log m(x) \sum_y W_{y,x} \right] 
\end{eqnarray}

Next we have the smoothing function for a given $\beta$

\begin{eqnarray}
f(x) &=& \frac{\log \beta}{\log(\beta + e^{x})} \\ c_x &=& f(H(x)) \\
&=& \log \beta \left[ \log \right( \beta + e^{H(x)} \left)\right]^{-1}
\end{eqnarray}


\section{References}
BibTeX is a pain, so for right now I'm going to do

\begin{verbatim}
@article{doi:10.2200/S00590ED1V01Y201408AIM029,
author = {  Amarnag 
  Subramanya  and   Partha Pratim 
  Talukdar },
title = {Graph-Based Semi-Supervised Learning},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
volume = {8},
number = {4},
pages = {1-125},
year = {2014},
doi = {10.2200/S00590ED1V01Y201408AIM029},

URL = { 
        https://doi.org/10.2200/S00590ED1V01Y201408AIM029
    
},
eprint = { 
        https://doi.org/10.2200/S00590ED1V01Y201408AIM029
    
}

}
\end{verbatim}

\end{document}